{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "45rVCxcuVyIb"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "goy1Us-0Vkva"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import tqdm\n",
    "import h5py\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZouIl31aV3xD"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1844,
     "status": "ok",
     "timestamp": 1563172769106,
     "user": {
      "displayName": "Andrea Amico",
      "photoUrl": "https://lh6.googleusercontent.com/-dSwndag-hMI/AAAAAAAAAAI/AAAAAAAAAAc/PC1EqxEQSfM/s64/photo.jpg",
      "userId": "00481473970919963641"
     },
     "user_tz": -120
    },
    "id": "7AlAdhGjVuor",
    "outputId": "a8c794f5-a6ad-429f-801a-6041f0f66ec6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from keras.callbacks import Callback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def progress_bar(current_value, max_value):\n",
    "    progress = ((current_value+1)/max_value)*100\n",
    "    if progress>98: progress=100\n",
    "    print('\\r[{0}{1}] {2:.1f}%'.format('#'*int(progress/2), ' '*(50-int(progress/2)), progress), end='')\n",
    "\n",
    "\n",
    "def play_bell():\n",
    "    import winsound\n",
    "    duration = 200  # millisecond\n",
    "    freq = 440  # Hz\n",
    "    for i in range(5):\n",
    "        winsound.Beep(int(freq*(i/2+1)), duration)\n",
    "    \n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def __init__(self, number_of_epochs, logging_parameter='acc'):\n",
    "        self.number_of_epochs = number_of_epochs\n",
    "        self.current_epoch = 0\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.initial_time = time.time()\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        if logs['batch']==0:\n",
    "            NUMBER_OF_DIESIS = 20\n",
    "            self.current_epoch += 1\n",
    "            progress = self.current_epoch/self.number_of_epochs\n",
    "            diesis = np.round(progress*NUMBER_OF_DIESIS).astype('int')\n",
    "            eta = (time.time()-self.initial_time) * (self.number_of_epochs/self.current_epoch -1)\n",
    "            remaining_time = time.strftime(\"%H hours, %M min, %S sec\", time.gmtime(eta))\n",
    "            print('\\r[{}{}]  eta: {}'.format(\n",
    "                '#'*diesis, '-'*(NUMBER_OF_DIESIS-diesis), remaining_time), end='')\n",
    "#             print('\\r[{}{}] {}: {:.3f} eta: {}'.format(\n",
    "#                 '#'*diesis, '-'*(NUMBER_OF_DIESIS-diesis), training_metric, logs[training_metric], remaining_time), end='')\n",
    "\n",
    "def add_grid_and_save(grid):\n",
    "    # save data in temporary dataframe if cross validation is not over\n",
    "    if len(grid['fit_outs'][1]) < grid['skf_n_splits'][1]:\n",
    "        temp_grid_file_path = '{}/grids/grid_tmp.pkl'.format(grid['root_path'][1])\n",
    "        grid_df = pd.DataFrame()\n",
    "        grid_df = grid_df.append({key:grid[key][1] for key in grid.keys()}, ignore_index=True)\n",
    "        grid_df.to_pickle(temp_grid_file_path)\n",
    "\n",
    "    else:\n",
    "        grid_file_path = '{}/grids/grid_{}.pkl'.format(grid['root_path'][1], grid['version'][1])\n",
    "        if os.path.isfile(grid_file_path):\n",
    "            grid_df = pd.read_pickle(grid_file_path)\n",
    "        else:\n",
    "            grid_df = pd.DataFrame()\n",
    "\n",
    "        grid_df = grid_df.append({key:grid[key][1] for key in grid.keys()}, ignore_index=True)\n",
    "        grid_df.to_pickle(grid_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tx-Im5pNV87D"
   },
   "source": [
    "### Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pzhsrqRuVurC"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(grid):\n",
    "    train_data_filename = f'{ROOT_PATH}/data/train_data_sorted.h5'\n",
    "\n",
    "    with h5py.File(train_data_filename, 'r') as f:\n",
    "        X_train = f['X_train'][()]\n",
    "        y_train = f['y_train'][()]\n",
    "\n",
    "    dev_data_filename = f'{ROOT_PATH}/data/dev_data_sorted.h5'\n",
    "    with h5py.File(dev_data_filename, 'r') as f:\n",
    "        X_dev = f['X_dev'][()]\n",
    "        y_dev = f['y_dev'][()]\n",
    "\n",
    "\n",
    "    # from sklearn.preprocessing import Normalizer\n",
    "    # transformer = Normalizer().fit(X_train)\n",
    "\n",
    "    # X_train = transformer.transform(X_train)\n",
    "    # X_dev = transformer.transform(X_dev)\n",
    "\n",
    "    # y_max = np.max(y_train)\n",
    "    # y_train = y_train/y_max\n",
    "    # y_dev = y_dev/y_max\n",
    "\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1, 1)\n",
    "    X_dev = X_dev.reshape(X_dev.shape[0], -1, 1)\n",
    "    return X_train, X_dev, y_train, y_dev\n",
    "\n",
    "\n",
    "def pre_process_data(X, y, grid):\n",
    "    ### TODO\n",
    "    return X_processed, y_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LXYl5ZB1WDsr"
   },
   "source": [
    "### Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W_OVdEIrVutO"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "from keras.layers.convolutional import Conv1D, SeparableConv1D\n",
    "from keras.layers.convolutional import MaxPooling1D, AveragePooling1D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import CuDNNGRU, LSTM\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "def create_model(grid):\n",
    "    model = Sequential()\n",
    "    \n",
    "    if grid['avg_pool'][1]:\n",
    "        model.add(AveragePooling1D(pool_size=grid['avg_pool_size'][1]))\n",
    "    \n",
    "    for conv_layer_index in range(grid['conv_layers_count'][1]):\n",
    "        model.add(SeparableConv1D(filters=grid[f'conv_{conv_layer_index}'][1], kernel_size=3, padding='same', activation='relu', ))\n",
    "        model.add(MaxPooling1D(pool_size=grid[f'pool_{conv_layer_index}'][1]))\n",
    "        \n",
    "    if grid['gru'][1]:\n",
    "        model.add(CuDNNGRU(grid['gru_size'][1])) \n",
    "    else:\n",
    "        model.add(Flatten())\n",
    "    model.add(Dense(grid['dense_size'][1], activation='relu', kernel_initializer='normal'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WVKLze70v_rB"
   },
   "source": [
    "### Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T-fZcZuHYaKe"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from scipy.stats import kde\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "def train_model(X, y, model, grid):\n",
    "    skf = KFold(n_splits=grid['skf_n_splits'][1], shuffle=True)\n",
    "    \n",
    "    train_index, dev_index = list(skf.split(X, y))[len(grid['fit_outs'][1])]\n",
    "    \n",
    "    X_train, X_dev = X[train_index], X[dev_index]\n",
    "    y_train, y_dev = y[train_index], y[dev_index]\n",
    "    \n",
    "\n",
    "    progress_bar = LossHistory(grid['epochs'][1], logging_parameter=grid['training_metric'][1])\n",
    "    chk = ModelCheckpoint(grid['best_model_paths'][1][-1], monitor=grid['best_model_metric'][1],\n",
    "                          save_best_only=True, mode='max', verbose=0)\n",
    "    \n",
    "    optimizer = optimizers.Adam(lr=grid['learning_rate'][1])\n",
    "    model.compile(loss=grid['loss'][1], optimizer=optimizer, metrics=[grid['training_metric'][1]])\n",
    "    fit_out = model.fit(X_train, y_train, epochs=grid['epochs'][1], batch_size=grid['batch_size'][1],\n",
    "                        callbacks=[ progress_bar], validation_data=(X_dev, y_dev), verbose=0) # removed chk\n",
    "\n",
    "    fit_out.model = None\n",
    "    fit_out.epoch = None\n",
    "    fit_out.validation_data = None\n",
    "    grid['fit_outs'][1].append(fit_out)\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6VRHyhoTWPli"
   },
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NYo6VCM-VuvU"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def run_training(X, y, grid, verbose=True):\n",
    "    initial_time = time.time()\n",
    "    \n",
    "    for index_split in range(grid['skf_n_splits'][1]): # looping on uncompleted CV trainings\n",
    "        print(f\"\\nCV validation {index_split+1} of {grid['skf_n_splits'][1]}\")\n",
    "\n",
    "        model = create_model(grid)\n",
    "\n",
    "        best_model_path = '{}/models/{}/best_model_{}_{}.pkl'.format(grid['root_path'][1], grid['version'][1], grid['test_index'][1], index_split)\n",
    "        grid['best_model_paths'][1].append(best_model_path)\n",
    "              \n",
    "        grid = train_model(X, y, model, grid)\n",
    "        grid['best_model_results'][1].append(np.max(grid['fit_outs'][1][-1].history[grid['best_model_metric'][1]]))\n",
    "        add_grid_and_save(grid)\n",
    "\n",
    "    total_time = time.strftime(\"%H hours, %M min, %S sec\", time.gmtime((time.time() - initial_time)))\n",
    "    print('  --  Model trained in {}'.format(total_time))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xl7jzmg2WPG5"
   },
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zw_r6DcCercE"
   },
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25973,
     "status": "ok",
     "timestamp": 1563172798258,
     "user": {
      "displayName": "Andrea Amico",
      "photoUrl": "https://lh6.googleusercontent.com/-dSwndag-hMI/AAAAAAAAAAI/AAAAAAAAAAc/PC1EqxEQSfM/s64/photo.jpg",
      "userId": "00481473970919963641"
     },
     "user_tz": -120
    },
    "id": "GgAYdcATVuxj",
    "outputId": "6e6188e7-7391-4df6-9d4b-3b78aa5091b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/gdrive\n",
      "Working on google colab\n"
     ]
    }
   ],
   "source": [
    "VERSION = '04'\n",
    "\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/gdrive', force_remount=True)\n",
    "    ROOT_PATH = 'gdrive/My Drive/Colab Notebooks/LANL' # project path in drive\n",
    "    print('Working on google colab')\n",
    "except:\n",
    "    ROOT_PATH = '..'\n",
    "    print('Working locally')\n",
    "\n",
    "# where models and grids are saved\n",
    "directories = [f'{ROOT_PATH}/models/{VERSION}', f'{ROOT_PATH}/grids/']\n",
    "for directory in directories:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LOI7227FwIo8"
   },
   "source": [
    "### Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aCoMUmTNVuz9"
   },
   "outputs": [],
   "source": [
    "def create_grid(params={}):\n",
    "    \n",
    "    grid_file_path = '{}/grids/grid_{}.pkl'.format(ROOT_PATH, VERSION)\n",
    "    \n",
    "    if os.path.isfile(grid_file_path):\n",
    "        grid_df = pd.read_pickle(grid_file_path)\n",
    "        test_index = grid_df['test_index'].max() + 1\n",
    "    else:\n",
    "        test_index = 0\n",
    "\n",
    "    current_grid = {\n",
    "        'version'                : ['str'     , VERSION],\n",
    "        'params'                 : ['O'       , params],\n",
    "        'test_index'             : [np.int    , test_index],\n",
    "        'root_path'              : ['str'     , ROOT_PATH],\n",
    "        \n",
    "        # Model\n",
    "        'avg_pool'               : ['bool'     , True],\n",
    "        'avg_pool_size'          : [np.int     , 2],\n",
    "        'conv_layers_count'      : [np.int     , 3],\n",
    "        'filter_0'               : [np.int     , 8],\n",
    "        \n",
    "        'conv_0'                 : [np.int     , 8],\n",
    "        'pool_0'                 : [np.int     , 16],\n",
    "        'conv_1'                 : [np.int     , 16],\n",
    "        'pool_1'                 : [np.int     , 8],\n",
    "        'conv_2'                 : [np.int     , 32],\n",
    "        'pool_2'                 : [np.int     , 2],\n",
    "\n",
    "        \n",
    "        'dense_size'             : [np.int     , 50],\n",
    "        'gru'                    : ['bool'     , True],\n",
    "        'gru_size'               : [np.int     , 50],\n",
    "        \n",
    "        # Metrics\n",
    "        'best_model_metric'      : ['str'     , 'val_loss'],\n",
    "        'training_metric'        : ['str'     , 'mean_squared_error'],\n",
    "        'loss'                   : ['str'     , 'mean_squared_error'],\n",
    "        \n",
    "        'learning_rate'          : [np.float  , 0.001],\n",
    "        \n",
    "        # Training parameters\n",
    "        'epochs'                 : [np.int    , 100],\n",
    "        'batch_size'             : [np.int    , 32],\n",
    "        'skf_n_splits'           : [np.int    , 5],\n",
    "        \n",
    "        # Outputs\n",
    "        'best_model_paths'       : ['O'       , []],\n",
    "        'best_model_results'     : ['O'       , []],\n",
    "        'fit_outs'               : ['O'       , []]}\n",
    "\n",
    "    for key, value in params.items():\n",
    "        current_grid[key][1] = value\n",
    "    return current_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JH8uVp41euNC"
   },
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3gjVw9swol3Q"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_data({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qRJvJHlaWwBq"
   },
   "outputs": [],
   "source": [
    "def test_params(next_point_to_probe, X_train, X_test, y_train, y_test):\n",
    "    with open (f'{ROOT_PATH}/log.txt', 'a') as f:\n",
    "        f.write(f'{time.ctime()}  -  starting {next_point_to_probe}\\n')\n",
    "    print(f'Testing {next_point_to_probe}')\n",
    "\n",
    "    \n",
    "    grid = create_grid(next_point_to_probe)\n",
    "\n",
    "    grid = run_training(X_train, y_train, grid, verbose=True)\n",
    "    \n",
    "    val_loss_history = [fit_out.history['val_loss'] for fit_out in grid['fit_outs'][1]]\n",
    "    val_loss_estimator = np.mean(val_loss_history, 0)\n",
    "    val_loss_estimator.sort()\n",
    "    metric_result = val_loss_estimator[:10].mean()\n",
    "    \n",
    "    \n",
    "    with open (f'{ROOT_PATH}/log.txt', 'a') as f:\n",
    "        f.write(f'{time.ctime()}  -  finished {next_point_to_probe}\\n')\n",
    "    \n",
    "    return metric_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AsRGA9vFs-BT"
   },
   "outputs": [],
   "source": [
    "def get_next_point():\n",
    "    point = {}\n",
    "    \n",
    "    point['loss'] = np.random.choice(['mean_squared_error', 'mean_absolute_error'])\n",
    "    point['learning_rate'] = np.power(10, np.random.uniform(-5, -3))\n",
    "    point['avg_pool'] = np.random.choice([True, False])\n",
    "    if point['avg_pool']:\n",
    "        point['avg_pool_size'] = np.random.randint(2, 8)\n",
    "        \n",
    "    point['conv_layers_count'] = np.random.randint(2, 4)\n",
    "    point['gru'] = np.random.choice([True, False])\n",
    "    \n",
    "    point['gru_size'] = np.random.randint(20, 100)\n",
    "    point['dense_size'] = np.random.randint(20, 100)\n",
    "    return point\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13441667,
     "status": "error",
     "timestamp": 1563186263817,
     "user": {
      "displayName": "Andrea Amico",
      "photoUrl": "https://lh6.googleusercontent.com/-dSwndag-hMI/AAAAAAAAAAI/AAAAAAAAAAc/PC1EqxEQSfM/s64/photo.jpg",
      "userId": "00481473970919963641"
     },
     "user_tz": -120
    },
    "id": "JgS7lnEkc58q",
    "outputId": "0342ab13-c739-4db3-a320-2234a15edec6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test 0\n",
      "Testing {'loss': 'mean_squared_error', 'learning_rate': 4.9624694077469894e-05, 'avg_pool': False, 'conv_layers_count': 3, 'gru': True, 'gru_size': 95, 'dense_size': 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0715 06:40:33.785174 140408954263424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CV validation 1 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0715 06:40:34.625317 140408954263424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0715 06:40:34.626812 140408954263424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0715 06:40:34.631552 140408954263424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0715 06:40:34.677778 140408954263424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0715 06:40:36.585258 140408954263424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "W0715 06:40:36.935292 140408954263424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 2 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 3 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 4 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 5 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec  --  Model trained in 00 hours, 26 min, 30 sec\n",
      "Starting test 1\n",
      "Testing {'loss': 'mean_squared_error', 'learning_rate': 4.802498102756505e-05, 'avg_pool': False, 'conv_layers_count': 2, 'gru': False, 'gru_size': 42, 'dense_size': 82}\n",
      "\n",
      "CV validation 1 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 2 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 3 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 4 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 5 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec  --  Model trained in 00 hours, 20 min, 20 sec\n",
      "Starting test 2\n",
      "Testing {'loss': 'mean_absolute_error', 'learning_rate': 5.3880514807643305e-05, 'avg_pool': False, 'conv_layers_count': 2, 'gru': True, 'gru_size': 38, 'dense_size': 80}\n",
      "\n",
      "CV validation 1 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 2 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 3 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 4 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 5 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec  --  Model trained in 00 hours, 29 min, 29 sec\n",
      "Starting test 3\n",
      "Testing {'loss': 'mean_squared_error', 'learning_rate': 0.000418940753541257, 'avg_pool': False, 'conv_layers_count': 2, 'gru': True, 'gru_size': 89, 'dense_size': 70}\n",
      "\n",
      "CV validation 1 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 2 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 3 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 4 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 5 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec  --  Model trained in 00 hours, 32 min, 00 sec\n",
      "Starting test 4\n",
      "Testing {'loss': 'mean_squared_error', 'learning_rate': 3.7111364921369e-05, 'avg_pool': True, 'avg_pool_size': 7, 'conv_layers_count': 3, 'gru': True, 'gru_size': 96, 'dense_size': 20}\n",
      "\n",
      "CV validation 1 of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0715 08:28:55.235006 140408954263424 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3980: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 2 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 3 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 4 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 5 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec  --  Model trained in 00 hours, 12 min, 25 sec\n",
      "Starting test 5\n",
      "Testing {'loss': 'mean_absolute_error', 'learning_rate': 0.00030478683507330866, 'avg_pool': False, 'conv_layers_count': 2, 'gru': True, 'gru_size': 43, 'dense_size': 61}\n",
      "\n",
      "CV validation 1 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 2 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 3 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 4 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 5 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec  --  Model trained in 00 hours, 30 min, 32 sec\n",
      "Starting test 6\n",
      "Testing {'loss': 'mean_absolute_error', 'learning_rate': 7.251954131761099e-05, 'avg_pool': True, 'avg_pool_size': 3, 'conv_layers_count': 3, 'gru': True, 'gru_size': 96, 'dense_size': 79}\n",
      "\n",
      "CV validation 1 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 2 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 3 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 4 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 5 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec  --  Model trained in 00 hours, 15 min, 29 sec\n",
      "Starting test 7\n",
      "Testing {'loss': 'mean_absolute_error', 'learning_rate': 7.74976943716483e-05, 'avg_pool': False, 'conv_layers_count': 3, 'gru': True, 'gru_size': 77, 'dense_size': 75}\n",
      "\n",
      "CV validation 1 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 2 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 3 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 4 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 5 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec  --  Model trained in 00 hours, 27 min, 11 sec\n",
      "Starting test 8\n",
      "Testing {'loss': 'mean_absolute_error', 'learning_rate': 8.756099753703054e-05, 'avg_pool': False, 'conv_layers_count': 3, 'gru': True, 'gru_size': 53, 'dense_size': 25}\n",
      "\n",
      "CV validation 1 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 2 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 3 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 4 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec\n",
      "CV validation 5 of 5\n",
      "[####################]  eta: 00 hours, 00 min, 00 sec  --  Model trained in 00 hours, 27 min, 41 sec\n",
      "Starting test 9\n",
      "Testing {'loss': 'mean_absolute_error', 'learning_rate': 0.00013080564964663714, 'avg_pool': False, 'conv_layers_count': 2, 'gru': True, 'gru_size': 67, 'dense_size': 60}\n",
      "\n",
      "CV validation 1 of 5\n",
      "[#######-------------]  eta: 00 hours, 04 min, 23 sec"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d35dc766369a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mnext_point_to_probe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_next_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_point_to_probe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-00b29864e5eb>\u001b[0m in \u001b[0;36mtest_params\u001b[0;34m(next_point_to_probe, X_train, X_test, y_train, y_test)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_point_to_probe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mval_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfit_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfit_out\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fit_outs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d4cda4ddb4ee>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(X, y, grid, verbose)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_model_paths'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_model_results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fit_outs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_model_metric'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0madd_grid_and_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ffaebaf4f2a1>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(X, y, model, grid)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training_metric'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     fit_out = model.fit(X_train, y_train, epochs=grid['epochs'][1], batch_size=grid['batch_size'][1],\n\u001b[0;32m---> 28\u001b[0;31m                         callbacks=[ progress_bar], validation_data=(X_dev, y_dev), verbose=0) # removed chk\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mfit_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    185\u001b[0m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[1;32m    186\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                     raise TypeError('TypeError while preparing batch. '\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for random_search_index in range(10):\n",
    "    print(f'Starting test {random_search_index}')\n",
    "    next_point_to_probe = get_next_point()\n",
    "\n",
    "    target = test_params(next_point_to_probe, X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g7klz-Odtc61"
   },
   "outputs": [],
   "source": [
    " +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QIBseWwEtc9X"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model_04.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
